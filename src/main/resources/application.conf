app {
  server {
    host = "0.0.0.0"
    port = 8080
  }
  
  llm {
    # Backend: "bedrock" or "ollama"
    backend = "ollama"
    
    bedrock {
      region = "us-east-1"
      # Model ID for Claude 3 Sonnet
      model = "anthropic.claude-3-sonnet-20240229-v1:0"
    }
    
    ollama {
      # Ollama endpoint
      endpoint = "http://localhost:11434"
      # Available models: llama2, neural-chat, starling-lm, etc.
      model = "llama2"
    }
  }
}
